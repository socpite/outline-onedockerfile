#!/bin/bash
# Outline Import Script
# Imports Outline workspace from an exported folder

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
IMPORT_DIR=""
FORCE=false
SKIP_FILES=false
SKIP_DATABASE=false
VERBOSE=false
DRY_RUN=false

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

usage() {
    echo "Usage: $0 [OPTIONS]"
    echo ""
    echo "Import Outline workspace data from exported folder"
    echo ""
    echo "Options:"
    echo "  -d, --dir DIR         Import directory (required)"
    echo "  -f, --force          Force import (overwrite existing data)"
    echo "  --skip-files         Skip file attachments import"
    echo "  --skip-database      Skip database import"
    echo "  --dry-run            Show what would be imported without doing it"
    echo "  -v, --verbose        Verbose output"
    echo "  -h, --help           Show this help"
    echo ""
    echo "Examples:"
    echo "  $0 -d /tmp/outline-export"
    echo "  $0 -d ./backup --force --verbose"
    echo "  $0 -d ./backup --skip-files --dry-run"
}

log() {
    if [[ "$VERBOSE" == true ]]; then
        echo -e "${BLUE}[INFO]${NC} $1"
    fi
}

success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

error() {
    echo -e "${RED}[ERROR]${NC} $1"
    exit 1
}

check_dependencies() {
    log "Checking dependencies..."
    
    if ! command -v psql &> /dev/null; then
        error "psql not found. Please install PostgreSQL client."
    fi
    
    if [[ -f "$IMPORT_DIR/workspace.json" ]]; then
        if ! command -v jq &> /dev/null; then
            error "jq not found. Please install jq for JSON import."
        fi
    fi
}

validate_import_directory() {
    log "Validating import directory..."
    
    if [[ ! -d "$IMPORT_DIR" ]]; then
        error "Import directory does not exist: $IMPORT_DIR"
    fi
    
    # Check for export metadata
    if [[ ! -f "$IMPORT_DIR/export_metadata.json" ]]; then
        warn "Export metadata not found. This may not be a valid Outline export."
    fi
    
    # Check for at least one data file
    if [[ ! -f "$IMPORT_DIR/database.sql" && ! -f "$IMPORT_DIR/workspace.json" ]]; then
        error "No database files found. Expected database.sql or workspace.json"
    fi
    
    success "Import directory validated"
}

check_database_connection() {
    log "Checking database connection..."
    
    if ! psql "$DATABASE_URL" -c "SELECT 1;" &> /dev/null; then
        error "Cannot connect to database. Check DATABASE_URL environment variable."
    fi
    
    success "Database connection verified"
}

check_existing_data() {
    log "Checking for existing data..."
    
    local team_count=$(psql "$DATABASE_URL" -t -c "SELECT COUNT(*) FROM teams;" 2>/dev/null | tr -d ' ' || echo "0")
    local user_count=$(psql "$DATABASE_URL" -t -c "SELECT COUNT(*) FROM users;" 2>/dev/null | tr -d ' ' || echo "0")
    local doc_count=$(psql "$DATABASE_URL" -t -c "SELECT COUNT(*) FROM documents;" 2>/dev/null | tr -d ' ' || echo "0")
    
    if [[ "$team_count" -gt 0 || "$user_count" -gt 0 || "$doc_count" -gt 0 ]]; then
        warn "Existing data found: $team_count teams, $user_count users, $doc_count documents"
        
        if [[ "$FORCE" != true ]]; then
            echo ""
            echo "⚠️  WARNING: This will overwrite existing data!"
            echo "   Teams: $team_count"
            echo "   Users: $user_count" 
            echo "   Documents: $doc_count"
            echo ""
            read -p "Continue? (y/N): " -n 1 -r
            echo
            if [[ ! $REPLY =~ ^[Yy]$ ]]; then
                echo "Import cancelled."
                exit 0
            fi
        fi
    fi
}

backup_existing_data() {
    if [[ "$DRY_RUN" == true ]]; then
        log "[DRY RUN] Would backup existing database"
        return
    fi
    
    log "Creating backup of existing data..."
    
    local backup_file="/tmp/outline_backup_$(date +%Y%m%d_%H%M%S).sql"
    
    if pg_dump "$DATABASE_URL" > "$backup_file" 2>/dev/null; then
        success "Backup created: $backup_file"
    else
        warn "Failed to create backup, continuing anyway..."
    fi
}

import_database_sql() {
    local sql_file="$IMPORT_DIR/database.sql"
    
    if [[ ! -f "$sql_file" ]]; then
        log "No SQL file found, skipping SQL import"
        return
    fi
    
    if [[ "$DRY_RUN" == true ]]; then
        log "[DRY RUN] Would import database from: $sql_file"
        return
    fi
    
    log "Importing database from SQL: $sql_file"
    
    # Drop existing data if force is enabled
    if [[ "$FORCE" == true ]]; then
        log "Dropping existing tables..."
        psql "$DATABASE_URL" -c "DROP SCHEMA public CASCADE; CREATE SCHEMA public;" &>/dev/null || true
    fi
    
    if psql "$DATABASE_URL" < "$sql_file"; then
        success "Database imported from SQL"
    else
        error "Failed to import database from SQL"
    fi
}

import_database_json() {
    local json_file="$IMPORT_DIR/workspace.json"
    
    if [[ ! -f "$json_file" ]]; then
        log "No JSON file found, skipping JSON import"
        return
    fi
    
    if [[ "$DRY_RUN" == true ]]; then
        log "[DRY RUN] Would import database from: $json_file"
        return
    fi
    
    log "Importing database from JSON: $json_file"
    
    # Check if python3 is available for JSON parsing
    if ! command -v python3 &> /dev/null; then
        error "python3 is required for JSON import. Please install python3."
    fi
    
    # Create Python script for import
    cat > /tmp/import_json.py << 'PYTHON_SCRIPT'
import json
import sys
import subprocess
import os

def run_sql(sql, database_url):
    """Execute SQL command using psql"""
    try:
        result = subprocess.run(['psql', database_url, '-c', sql], 
                              capture_output=True, text=True, check=True)
        return True, result.stdout
    except subprocess.CalledProcessError as e:
        return False, e.stderr

def escape_sql_value(value):
    """Escape SQL values properly"""
    if value is None:
        return 'NULL'
    elif isinstance(value, bool):
        return 'true' if value else 'false'
    elif isinstance(value, (int, float)):
        return str(value)
    elif isinstance(value, (dict, list)):
        return "'" + json.dumps(value).replace("'", "''") + "'"
    else:
        return "'" + str(value).replace("'", "''") + "'"

def import_table(table_name, records, database_url, force_mode):
    """Import records into a table"""
    if not records:
        print(f"No data found for table: {table_name}")
        return True, {}
    
    print(f"Importing {len(records)} records to {table_name}...")
    
    # Clear existing data if force mode
    if force_mode:
        success, _ = run_sql(f'TRUNCATE TABLE "{table_name}" CASCADE;', database_url)
        if not success:
            print(f"Warning: Could not truncate {table_name}")
    
    # Import each record
    success_count = 0
    for record in records:
        if not record:
            continue
            
        # Build INSERT statement
        columns = list(record.keys())
        values = [escape_sql_value(record[col]) for col in columns]
        
        # Quote column names to handle reserved words
        quoted_columns = [f'"{col}"' for col in columns]
        
        sql = f'''INSERT INTO "{table_name}" ({', '.join(quoted_columns)}) 
                  VALUES ({', '.join(values)}) 
                  ON CONFLICT (id) DO UPDATE SET 
                  {', '.join([f'"{col}" = EXCLUDED."{col}"' for col in columns if col != 'id'])};'''
        
        success, error = run_sql(sql, database_url)
        if success:
            success_count += 1
        else:
            print(f"Warning: Failed to import record in {table_name}: {error}")
    
    print(f"✓ Imported {success_count}/{len(records)} records to {table_name}")
    return success_count > 0, {}

def main():
    if len(sys.argv) != 4:
        print("Usage: python3 import_json.py <json_file> <database_url> <force_mode>")
        sys.exit(1)
    
    json_file = sys.argv[1]
    database_url = sys.argv[2]
    force_mode = sys.argv[3].lower() == 'true'
    
    # Load JSON data
    try:
        with open(json_file, 'r') as f:
            data = json.load(f)
    except Exception as e:
        print(f"Error loading JSON: {e}")
        sys.exit(1)
    
    # Import tables in dependency order
    tables = ["teams", "users", "collections", "groups"]
    
    # Import core tables first
    for table in tables:
        records = data.get(table, [])
        success, _ = import_table(table, records, database_url, force_mode)
    
    # Special handling for documents - fix null collectionIds
    documents = data.get("documents", [])
    if documents:
        # Get the first available collection from database
        success, result = run_sql('SELECT id FROM collections LIMIT 1;', database_url)
        if success and result:
            lines = result.strip().split('\n')
            if len(lines) >= 3:  # Skip header and separator
                first_collection_id = lines[2].strip()
                print(f"Found collection in database: {first_collection_id}")
                
                # Fix documents with null collectionId
                for record in documents:
                    if record.get("collectionId") is None:
                        print(f"Fixing document '{record.get('title', 'Unknown')}' - assigning to collection {first_collection_id}")
                        record["collectionId"] = first_collection_id
                    
                    # Check for missing content and fix it
                    if record.get("content") is None and record.get("text"):
                        print(f"Warning: Document '{record.get('title', 'Unknown')}' has missing content, generating from text")
                        # Create basic ProseMirror structure from text
                        text_content = record.get("text", "")
                        record["content"] = {
                            "type": "doc",
                            "content": [
                                {
                                    "type": "paragraph",
                                    "content": [
                                        {
                                            "type": "text",
                                            "text": text_content
                                        }
                                    ]
                                }
                            ]
                        }
                    elif record.get("content") is None:
                        print(f"Warning: Document '{record.get('title', 'Unknown')}' has no content or text")
                        record["content"] = {
                            "type": "doc",
                            "content": [{"type": "paragraph"}]
                        }
        
        # Import documents
        success, _ = import_table("documents", documents, database_url, force_mode)
    
    # Import remaining tables
    remaining_tables = ["attachments", "shares", "stars", "pins", "views", "memberships", "group_users"]
    for table in remaining_tables:
        records = data.get(table, [])
        success, _ = import_table(table, records, database_url, force_mode)
    
    print("Database import completed!")

if __name__ == "__main__":
    main()
PYTHON_SCRIPT
    
    # Run the Python import script
    if python3 /tmp/import_json.py "$json_file" "$DATABASE_URL" "$FORCE"; then
        success "Database imported from JSON"
    else
        error "Failed to import database from JSON"
    fi
    
    # Clean up
    rm -f /tmp/import_json.py
}

fix_database_issues() {
    if [[ "$DRY_RUN" == true ]]; then
        log "[DRY RUN] Would fix database consistency issues"
        return
    fi
    
    log "Fixing database consistency issues..."
    
    # Fix NULL array fields that should be empty arrays
    local fixes_applied=0
    
    # Fix collaboratorIds in documents table
    local collaborator_count=$(psql "$DATABASE_URL" -t -c "SELECT COUNT(*) FROM documents WHERE \"collaboratorIds\" IS NULL;" 2>/dev/null | tr -d ' ' || echo "0")
    if [[ "$collaborator_count" -gt 0 ]]; then
        psql "$DATABASE_URL" -c "UPDATE documents SET \"collaboratorIds\" = '{}' WHERE \"collaboratorIds\" IS NULL;" >/dev/null 2>&1
        log "Fixed $collaborator_count documents with NULL collaboratorIds"
        fixes_applied=$((fixes_applied + collaborator_count))
    fi
    
    # Fix other common NULL array fields
    local tables_and_fields=(
        "users:notificationSettings"
        "collections:membershipIds" 
        "teams:allowedDomains"
        "documents:templateId"
    )
    
    for table_field in "${tables_and_fields[@]}"; do
        local table="${table_field%:*}"
        local field="${table_field#*:}"
        
        # Check if table and field exist before trying to fix
        local field_exists=$(psql "$DATABASE_URL" -t -c "SELECT COUNT(*) FROM information_schema.columns WHERE table_name='$table' AND column_name='$field';" 2>/dev/null | tr -d ' ' || echo "0")
        
        if [[ "$field_exists" -gt 0 ]]; then
            # Check field type to determine appropriate fix
            local field_type=$(psql "$DATABASE_URL" -t -c "SELECT data_type FROM information_schema.columns WHERE table_name='$table' AND column_name='$field';" 2>/dev/null | tr -d ' ' || echo "")
            
            if [[ "$field_type" == *"ARRAY"* || "$field_type" == *"array"* ]]; then
                # Fix array fields
                local null_count=$(psql "$DATABASE_URL" -t -c "SELECT COUNT(*) FROM \"$table\" WHERE \"$field\" IS NULL;" 2>/dev/null | tr -d ' ' || echo "0")
                if [[ "$null_count" -gt 0 ]]; then
                    psql "$DATABASE_URL" -c "UPDATE \"$table\" SET \"$field\" = '{}' WHERE \"$field\" IS NULL;" >/dev/null 2>&1
                    log "Fixed $null_count $table records with NULL $field"
                    fixes_applied=$((fixes_applied + null_count))
                fi
            elif [[ "$field_type" == *"json"* ]]; then
                # Fix JSON fields
                local null_count=$(psql "$DATABASE_URL" -t -c "SELECT COUNT(*) FROM \"$table\" WHERE \"$field\" IS NULL;" 2>/dev/null | tr -d ' ' || echo "0")
                if [[ "$null_count" -gt 0 ]]; then
                    psql "$DATABASE_URL" -c "UPDATE \"$table\" SET \"$field\" = '{}' WHERE \"$field\" IS NULL;" >/dev/null 2>&1
                    log "Fixed $null_count $table records with NULL $field"
                    fixes_applied=$((fixes_applied + null_count))
                fi
            fi
        fi
    done
    
    # Fix foreign key references that might be broken
    log "Checking foreign key consistency..."
    
    # Remove documents with invalid collection references
    local orphan_count=$(psql "$DATABASE_URL" -t -c "SELECT COUNT(*) FROM documents WHERE \"collectionId\" IS NOT NULL AND \"collectionId\" NOT IN (SELECT id FROM collections);" 2>/dev/null | tr -d ' ' || echo "0")
    if [[ "$orphan_count" -gt 0 ]]; then
        psql "$DATABASE_URL" -c "DELETE FROM documents WHERE \"collectionId\" IS NOT NULL AND \"collectionId\" NOT IN (SELECT id FROM collections);" >/dev/null 2>&1
        log "Removed $orphan_count orphaned documents"
        fixes_applied=$((fixes_applied + orphan_count))
    fi
    
    # Remove memberships with invalid user/team references
    local orphan_count=$(psql "$DATABASE_URL" -t -c "SELECT COUNT(*) FROM memberships WHERE \"userId\" NOT IN (SELECT id FROM users) OR \"teamId\" NOT IN (SELECT id FROM teams);" 2>/dev/null | tr -d ' ' || echo "0")
    if [[ "$orphan_count" -gt 0 ]]; then
        psql "$DATABASE_URL" -c "DELETE FROM memberships WHERE \"userId\" NOT IN (SELECT id FROM users) OR \"teamId\" NOT IN (SELECT id FROM teams);" >/dev/null 2>&1
        log "Removed $orphan_count orphaned memberships"
        fixes_applied=$((fixes_applied + orphan_count))
    fi
    
    if [[ "$fixes_applied" -gt 0 ]]; then
        success "Applied $fixes_applied database fixes"
    else
        log "No database issues found"
    fi
}

fix_user_permissions() {
    if [[ "$DRY_RUN" == true ]]; then
        log "[DRY RUN] Would fix user permissions for imported collections"
        return
    fi
    
    log "Setting up user permissions for imported collections..."
    
    # Get all users and collections
    local users=$(psql "$DATABASE_URL" -t -c "SELECT id FROM users;" 2>/dev/null | tr -d ' ' | grep -v '^$' || echo "")
    local collections=$(psql "$DATABASE_URL" -t -c "SELECT id FROM collections;" 2>/dev/null | tr -d ' ' | grep -v '^$' || echo "")
    
    if [[ -z "$users" ]]; then
        warn "No users found - skipping permission setup"
        return
    fi
    
    if [[ -z "$collections" ]]; then
        log "No collections found - skipping permission setup"
        return
    fi
    
    local permissions_added=0
    
    # For each collection, ensure at least one user has access
    while IFS= read -r collection_id; do
        [[ -z "$collection_id" ]] && continue
        
        # Check if any user already has permission to this collection
        local existing_perms=$(psql "$DATABASE_URL" -t -c "SELECT COUNT(*) FROM user_permissions WHERE \"collectionId\" = '$collection_id';" 2>/dev/null | tr -d ' ' || echo "0")
        
        if [[ "$existing_perms" -eq 0 ]]; then
            # No permissions exist, grant access to the first user (usually admin)
            local first_user=$(echo "$users" | head -n1)
            
            if [[ -n "$first_user" ]]; then
                psql "$DATABASE_URL" -c "
                INSERT INTO user_permissions (\"collectionId\", \"userId\", permission, \"createdById\", \"createdAt\", \"updatedAt\") 
                VALUES ('$collection_id', '$first_user', 'read_write', '$first_user', NOW(), NOW())
                ON CONFLICT DO NOTHING;" >/dev/null 2>&1
                
                if [[ $? -eq 0 ]]; then
                    permissions_added=$((permissions_added + 1))
                fi
            fi
        fi
    done <<< "$collections"
    
    if [[ "$permissions_added" -gt 0 ]]; then
        success "Added permissions for $permissions_added collections"
        log "Users can now access imported collections"
    else
        log "All collections already have user permissions"
    fi
}

import_files() {
    if [[ "$SKIP_FILES" == true ]]; then
        log "Skipping file attachments import"
        return
    fi
    
    local files_dir="$IMPORT_DIR/files"
    
    if [[ ! -d "$files_dir" ]]; then
        log "No files directory found, skipping file import"
        return
    fi
    
    if [[ "$DRY_RUN" == true ]]; then
        log "[DRY RUN] Would import files from: $files_dir"
        return
    fi
    
    log "Importing file attachments from: $files_dir"
    
    # Default file storage location
    local storage_dir="/var/lib/outline/data"
    
    # Create storage directory if it doesn't exist
    mkdir -p "$storage_dir"
    
    if cp -r "$files_dir"/* "$storage_dir"/ 2>/dev/null; then
        # Fix permissions
        chown -R ubuntu:ubuntu "$storage_dir" 2>/dev/null || true
        chmod -R 755 "$storage_dir" 2>/dev/null || true
        
        local size=$(du -sh "$storage_dir" | cut -f1)
        success "Files imported ($size)"
    else
        warn "No files found to import or copy failed"
    fi
}

show_import_summary() {
    echo ""
    echo "📊 Import Summary"
    echo "=================="
    
    if [[ -f "$IMPORT_DIR/export_metadata.json" ]]; then
        echo "📅 Export Date: $(node -p "JSON.parse(require('fs').readFileSync('$IMPORT_DIR/export_metadata.json')).exportedAt" 2>/dev/null || echo 'Unknown')"
        echo "🏷️  Export Version: $(node -p "JSON.parse(require('fs').readFileSync('$IMPORT_DIR/export_metadata.json')).version" 2>/dev/null || echo 'Unknown')"
    fi
    
    if [[ "$SKIP_DATABASE" != true ]]; then
        local team_count=$(psql "$DATABASE_URL" -t -c "SELECT COUNT(*) FROM teams;" 2>/dev/null | tr -d ' ' || echo "0")
        local user_count=$(psql "$DATABASE_URL" -t -c "SELECT COUNT(*) FROM users;" 2>/dev/null | tr -d ' ' || echo "0")
        local doc_count=$(psql "$DATABASE_URL" -t -c "SELECT COUNT(*) FROM documents;" 2>/dev/null | tr -d ' ' || echo "0")
        local collection_count=$(psql "$DATABASE_URL" -t -c "SELECT COUNT(*) FROM collections;" 2>/dev/null | tr -d ' ' || echo "0")
        
        echo "👥 Teams: $team_count"
        echo "👤 Users: $user_count"
        echo "📚 Collections: $collection_count"
        echo "📄 Documents: $doc_count"
    fi
    
    if [[ "$SKIP_FILES" != true && -d "/var/lib/outline/data" ]]; then
        local file_size=$(du -sh "/var/lib/outline/data" 2>/dev/null | cut -f1 || echo "0B")
        echo "📎 Files: $file_size"
    fi
}

# Parse command line arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        -d|--dir)
            IMPORT_DIR="$2"
            shift 2
            ;;
        -f|--force)
            FORCE=true
            shift
            ;;
        --skip-files)
            SKIP_FILES=true
            shift
            ;;
        --skip-database)
            SKIP_DATABASE=true
            shift
            ;;
        --dry-run)
            DRY_RUN=true
            shift
            ;;
        -v|--verbose)
            VERBOSE=true
            shift
            ;;
        -h|--help)
            usage
            exit 0
            ;;
        *)
            error "Unknown option: $1"
            ;;
    esac
done

# Validate arguments
if [[ -z "$IMPORT_DIR" ]]; then
    error "Import directory is required. Use -d or --dir option."
fi

# Check for DATABASE_URL
if [[ -z "$DATABASE_URL" ]]; then
    error "DATABASE_URL environment variable is required."
fi

# Main execution
echo "📥 Starting Outline import..."
echo "📁 Import directory: $IMPORT_DIR"
echo "💪 Force mode: $FORCE"
echo "🏃 Dry run: $DRY_RUN"
echo ""

check_dependencies
validate_import_directory
check_database_connection

if [[ "$SKIP_DATABASE" != true ]]; then
    check_existing_data
    backup_existing_data
fi

# Import database
if [[ "$SKIP_DATABASE" != true ]]; then
    # Try SQL first, then JSON
    if [[ -f "$IMPORT_DIR/database.sql" ]]; then
        import_database_sql
    elif [[ -f "$IMPORT_DIR/workspace.json" ]]; then
        import_database_json
    else
        error "No database file found to import"
    fi
    
    # Fix database consistency issues after import
    fix_database_issues
    
    # Fix user permissions for imported collections
    fix_user_permissions
fi

# Import files
import_files

# Show summary
if [[ "$DRY_RUN" != true ]]; then
    show_import_summary
    echo ""
    success "Import completed successfully!"
    echo ""
    echo "🚀 You can now start Outline and access your imported data."
    echo "🔗 Don't forget to update any configuration if needed."
else
    echo ""
    success "Dry run completed - no changes made"
fi