#!/bin/bash
# Outline Import Script
# Imports Outline workspace from an exported folder

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
IMPORT_DIR=""
FORCE=false
SKIP_FILES=false
SKIP_DATABASE=false
VERBOSE=false
DRY_RUN=false

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

usage() {
    echo "Usage: $0 [OPTIONS]"
    echo ""
    echo "Import Outline workspace data from exported folder"
    echo ""
    echo "Options:"
    echo "  -d, --dir DIR         Import directory (required)"
    echo "  -f, --force          Force import (overwrite existing data)"
    echo "  --skip-files         Skip file attachments import"
    echo "  --skip-database      Skip database import"
    echo "  --dry-run            Show what would be imported without doing it"
    echo "  -v, --verbose        Verbose output"
    echo "  -h, --help           Show this help"
    echo ""
    echo "Examples:"
    echo "  $0 -d /tmp/outline-export"
    echo "  $0 -d ./backup --force --verbose"
    echo "  $0 -d ./backup --skip-files --dry-run"
}

log() {
    if [[ "$VERBOSE" == true ]]; then
        echo -e "${BLUE}[INFO]${NC} $1"
    fi
}

success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

error() {
    echo -e "${RED}[ERROR]${NC} $1"
    exit 1
}

check_dependencies() {
    log "Checking dependencies..."
    
    if ! command -v psql &> /dev/null; then
        error "psql not found. Please install PostgreSQL client."
    fi
    
    if [[ -f "$IMPORT_DIR/workspace.json" ]]; then
        if ! command -v jq &> /dev/null; then
            error "jq not found. Please install jq for JSON import."
        fi
    fi
}

validate_import_directory() {
    log "Validating import directory..."
    
    if [[ ! -d "$IMPORT_DIR" ]]; then
        error "Import directory does not exist: $IMPORT_DIR"
    fi
    
    # Check for export metadata
    if [[ ! -f "$IMPORT_DIR/export_metadata.json" ]]; then
        warn "Export metadata not found. This may not be a valid Outline export."
    fi
    
    # Check for at least one data file
    if [[ ! -f "$IMPORT_DIR/database.sql" && ! -f "$IMPORT_DIR/workspace.json" ]]; then
        error "No database files found. Expected database.sql or workspace.json"
    fi
    
    success "Import directory validated"
}

check_database_connection() {
    log "Checking database connection..."
    
    if ! psql "$DATABASE_URL" -c "SELECT 1;" &> /dev/null; then
        error "Cannot connect to database. Check DATABASE_URL environment variable."
    fi
    
    success "Database connection verified"
}

check_existing_data() {
    log "Checking for existing data..."
    
    local team_count=$(psql "$DATABASE_URL" -t -c "SELECT COUNT(*) FROM teams;" 2>/dev/null | tr -d ' ' || echo "0")
    local user_count=$(psql "$DATABASE_URL" -t -c "SELECT COUNT(*) FROM users;" 2>/dev/null | tr -d ' ' || echo "0")
    local doc_count=$(psql "$DATABASE_URL" -t -c "SELECT COUNT(*) FROM documents;" 2>/dev/null | tr -d ' ' || echo "0")
    
    if [[ "$team_count" -gt 0 || "$user_count" -gt 0 || "$doc_count" -gt 0 ]]; then
        warn "Existing data found: $team_count teams, $user_count users, $doc_count documents"
        
        if [[ "$FORCE" != true ]]; then
            echo ""
            echo "⚠️  WARNING: This will overwrite existing data!"
            echo "   Teams: $team_count"
            echo "   Users: $user_count" 
            echo "   Documents: $doc_count"
            echo ""
            read -p "Continue? (y/N): " -n 1 -r
            echo
            if [[ ! $REPLY =~ ^[Yy]$ ]]; then
                echo "Import cancelled."
                exit 0
            fi
        fi
    fi
}

backup_existing_data() {
    if [[ "$DRY_RUN" == true ]]; then
        log "[DRY RUN] Would backup existing database"
        return
    fi
    
    log "Creating backup of existing data..."
    
    local backup_file="/tmp/outline_backup_$(date +%Y%m%d_%H%M%S).sql"
    
    if pg_dump "$DATABASE_URL" > "$backup_file" 2>/dev/null; then
        success "Backup created: $backup_file"
    else
        warn "Failed to create backup, continuing anyway..."
    fi
}

import_database_sql() {
    local sql_file="$IMPORT_DIR/database.sql"
    
    if [[ ! -f "$sql_file" ]]; then
        log "No SQL file found, skipping SQL import"
        return
    fi
    
    if [[ "$DRY_RUN" == true ]]; then
        log "[DRY RUN] Would import database from: $sql_file"
        return
    fi
    
    log "Importing database from SQL: $sql_file"
    
    # Drop existing data if force is enabled
    if [[ "$FORCE" == true ]]; then
        log "Dropping existing tables..."
        psql "$DATABASE_URL" -c "DROP SCHEMA public CASCADE; CREATE SCHEMA public;" &>/dev/null || true
    fi
    
    if psql "$DATABASE_URL" < "$sql_file"; then
        success "Database imported from SQL"
    else
        error "Failed to import database from SQL"
    fi
}

import_database_json() {
    local json_file="$IMPORT_DIR/workspace.json"
    
    if [[ ! -f "$json_file" ]]; then
        log "No JSON file found, skipping JSON import"
        return
    fi
    
    if [[ "$DRY_RUN" == true ]]; then
        log "[DRY RUN] Would import database from: $json_file"
        return
    fi
    
    log "Importing database from JSON: $json_file"
    
    # Check if python3 is available for JSON parsing
    if ! command -v python3 &> /dev/null; then
        error "python3 is required for JSON import. Please install python3."
    fi
    
    # Create Python script for import
    cat > /tmp/import_json.py << 'PYTHON_SCRIPT'
import json
import sys
import subprocess
import os

def run_sql(sql, database_url):
    """Execute SQL command using psql"""
    try:
        result = subprocess.run(['psql', database_url, '-c', sql], 
                              capture_output=True, text=True, check=True)
        return True, result.stdout
    except subprocess.CalledProcessError as e:
        return False, e.stderr

def escape_sql_value(value):
    """Escape SQL values properly"""
    if value is None:
        return 'NULL'
    elif isinstance(value, bool):
        return 'true' if value else 'false'
    elif isinstance(value, (int, float)):
        return str(value)
    elif isinstance(value, (dict, list)):
        return "'" + json.dumps(value).replace("'", "''") + "'"
    else:
        return "'" + str(value).replace("'", "''") + "'"

def import_table(table_name, records, database_url, force_mode):
    """Import records into a table"""
    if not records:
        print(f"No data found for table: {table_name}")
        return True
    
    print(f"Importing {len(records)} records to {table_name}...")
    
    # Clear existing data if force mode
    if force_mode:
        success, _ = run_sql(f'TRUNCATE TABLE "{table_name}" CASCADE;', database_url)
        if not success:
            print(f"Warning: Could not truncate {table_name}")
    
    # Import each record
    success_count = 0
    for record in records:
        if not record:
            continue
            
        # Build INSERT statement
        columns = list(record.keys())
        values = [escape_sql_value(record[col]) for col in columns]
        
        # Quote column names to handle reserved words
        quoted_columns = [f'"{col}"' for col in columns]
        
        sql = f'''INSERT INTO "{table_name}" ({', '.join(quoted_columns)}) 
                  VALUES ({', '.join(values)}) 
                  ON CONFLICT (id) DO UPDATE SET 
                  {', '.join([f'"{col}" = EXCLUDED."{col}"' for col in columns if col != 'id'])};'''
        
        success, error = run_sql(sql, database_url)
        if success:
            success_count += 1
        else:
            print(f"Warning: Failed to import record in {table_name}: {error}")
    
    print(f"✓ Imported {success_count}/{len(records)} records to {table_name}")
    return success_count > 0

def main():
    if len(sys.argv) != 4:
        print("Usage: python3 import_json.py <json_file> <database_url> <force_mode>")
        sys.exit(1)
    
    json_file = sys.argv[1]
    database_url = sys.argv[2]
    force_mode = sys.argv[3].lower() == 'true'
    
    # Load JSON data
    try:
        with open(json_file, 'r') as f:
            data = json.load(f)
    except Exception as e:
        print(f"Error loading JSON: {e}")
        sys.exit(1)
    
    # Import tables in dependency order
    tables = ["teams", "users", "collections", "groups", "documents", "attachments", 
              "shares", "stars", "pins", "views", "memberships", "group_users"]
    
    for table in tables:
        records = data.get(table, [])
        import_table(table, records, database_url, force_mode)
    
    print("Database import completed!")

if __name__ == "__main__":
    main()
PYTHON_SCRIPT
    
    # Run the Python import script
    if python3 /tmp/import_json.py "$json_file" "$DATABASE_URL" "$FORCE"; then
        success "Database imported from JSON"
    else
        error "Failed to import database from JSON"
    fi
    
    # Clean up
    rm -f /tmp/import_json.py
}

import_files() {
    if [[ "$SKIP_FILES" == true ]]; then
        log "Skipping file attachments import"
        return
    fi
    
    local files_dir="$IMPORT_DIR/files"
    
    if [[ ! -d "$files_dir" ]]; then
        log "No files directory found, skipping file import"
        return
    fi
    
    if [[ "$DRY_RUN" == true ]]; then
        log "[DRY RUN] Would import files from: $files_dir"
        return
    fi
    
    log "Importing file attachments from: $files_dir"
    
    # Default file storage location
    local storage_dir="/var/lib/outline/data"
    
    # Create storage directory if it doesn't exist
    mkdir -p "$storage_dir"
    
    if cp -r "$files_dir"/* "$storage_dir"/ 2>/dev/null; then
        # Fix permissions
        chown -R ubuntu:ubuntu "$storage_dir" 2>/dev/null || true
        chmod -R 755 "$storage_dir" 2>/dev/null || true
        
        local size=$(du -sh "$storage_dir" | cut -f1)
        success "Files imported ($size)"
    else
        warn "No files found to import or copy failed"
    fi
}

show_import_summary() {
    echo ""
    echo "📊 Import Summary"
    echo "=================="
    
    if [[ -f "$IMPORT_DIR/export_metadata.json" ]]; then
        echo "📅 Export Date: $(node -p "JSON.parse(require('fs').readFileSync('$IMPORT_DIR/export_metadata.json')).exportedAt" 2>/dev/null || echo 'Unknown')"
        echo "🏷️  Export Version: $(node -p "JSON.parse(require('fs').readFileSync('$IMPORT_DIR/export_metadata.json')).version" 2>/dev/null || echo 'Unknown')"
    fi
    
    if [[ "$SKIP_DATABASE" != true ]]; then
        local team_count=$(psql "$DATABASE_URL" -t -c "SELECT COUNT(*) FROM teams;" 2>/dev/null | tr -d ' ' || echo "0")
        local user_count=$(psql "$DATABASE_URL" -t -c "SELECT COUNT(*) FROM users;" 2>/dev/null | tr -d ' ' || echo "0")
        local doc_count=$(psql "$DATABASE_URL" -t -c "SELECT COUNT(*) FROM documents;" 2>/dev/null | tr -d ' ' || echo "0")
        local collection_count=$(psql "$DATABASE_URL" -t -c "SELECT COUNT(*) FROM collections;" 2>/dev/null | tr -d ' ' || echo "0")
        
        echo "👥 Teams: $team_count"
        echo "👤 Users: $user_count"
        echo "📚 Collections: $collection_count"
        echo "📄 Documents: $doc_count"
    fi
    
    if [[ "$SKIP_FILES" != true && -d "/var/lib/outline/data" ]]; then
        local file_size=$(du -sh "/var/lib/outline/data" 2>/dev/null | cut -f1 || echo "0B")
        echo "📎 Files: $file_size"
    fi
}

# Parse command line arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        -d|--dir)
            IMPORT_DIR="$2"
            shift 2
            ;;
        -f|--force)
            FORCE=true
            shift
            ;;
        --skip-files)
            SKIP_FILES=true
            shift
            ;;
        --skip-database)
            SKIP_DATABASE=true
            shift
            ;;
        --dry-run)
            DRY_RUN=true
            shift
            ;;
        -v|--verbose)
            VERBOSE=true
            shift
            ;;
        -h|--help)
            usage
            exit 0
            ;;
        *)
            error "Unknown option: $1"
            ;;
    esac
done

# Validate arguments
if [[ -z "$IMPORT_DIR" ]]; then
    error "Import directory is required. Use -d or --dir option."
fi

# Check for DATABASE_URL
if [[ -z "$DATABASE_URL" ]]; then
    error "DATABASE_URL environment variable is required."
fi

# Main execution
echo "📥 Starting Outline import..."
echo "📁 Import directory: $IMPORT_DIR"
echo "💪 Force mode: $FORCE"
echo "🏃 Dry run: $DRY_RUN"
echo ""

check_dependencies
validate_import_directory
check_database_connection

if [[ "$SKIP_DATABASE" != true ]]; then
    check_existing_data
    backup_existing_data
fi

# Import database
if [[ "$SKIP_DATABASE" != true ]]; then
    # Try SQL first, then JSON
    if [[ -f "$IMPORT_DIR/database.sql" ]]; then
        import_database_sql
    elif [[ -f "$IMPORT_DIR/workspace.json" ]]; then
        import_database_json
    else
        error "No database file found to import"
    fi
fi

# Import files
import_files

# Show summary
if [[ "$DRY_RUN" != true ]]; then
    show_import_summary
    echo ""
    success "Import completed successfully!"
    echo ""
    echo "🚀 You can now start Outline and access your imported data."
    echo "🔗 Don't forget to update any configuration if needed."
else
    echo ""
    success "Dry run completed - no changes made"
fi