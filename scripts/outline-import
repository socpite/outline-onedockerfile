#!/bin/bash
# Outline Import Script
# Imports Outline workspace from an exported folder

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
IMPORT_DIR=""
FORCE=false
SKIP_FILES=false
SKIP_DATABASE=false
VERBOSE=false
DRY_RUN=false

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

usage() {
    echo "Usage: $0 [OPTIONS]"
    echo ""
    echo "Import Outline workspace data from exported folder"
    echo ""
    echo "Options:"
    echo "  -d, --dir DIR         Import directory (required)"
    echo "  -f, --force          Force import (overwrite existing data)"
    echo "  --skip-files         Skip file attachments import"
    echo "  --skip-database      Skip database import"
    echo "  --dry-run            Show what would be imported without doing it"
    echo "  -v, --verbose        Verbose output"
    echo "  -h, --help           Show this help"
    echo ""
    echo "Examples:"
    echo "  $0 -d /tmp/outline-export"
    echo "  $0 -d ./backup --force --verbose"
    echo "  $0 -d ./backup --skip-files --dry-run"
}

log() {
    if [[ "$VERBOSE" == true ]]; then
        echo -e "${BLUE}[INFO]${NC} $1"
    fi
}

success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

error() {
    echo -e "${RED}[ERROR]${NC} $1"
    exit 1
}

check_dependencies() {
    log "Checking dependencies..."
    
    if ! command -v psql &> /dev/null; then
        error "psql not found. Please install PostgreSQL client."
    fi
    
    if [[ -f "$IMPORT_DIR/workspace.json" ]]; then
        if ! command -v node &> /dev/null; then
            error "node not found. Please install Node.js for JSON import."
        fi
    fi
}

validate_import_directory() {
    log "Validating import directory..."
    
    if [[ ! -d "$IMPORT_DIR" ]]; then
        error "Import directory does not exist: $IMPORT_DIR"
    fi
    
    # Check for export metadata
    if [[ ! -f "$IMPORT_DIR/export_metadata.json" ]]; then
        warn "Export metadata not found. This may not be a valid Outline export."
    fi
    
    # Check for at least one data file
    if [[ ! -f "$IMPORT_DIR/database.sql" && ! -f "$IMPORT_DIR/workspace.json" ]]; then
        error "No database files found. Expected database.sql or workspace.json"
    fi
    
    success "Import directory validated"
}

check_database_connection() {
    log "Checking database connection..."
    
    if ! psql "$DATABASE_URL" -c "SELECT 1;" &> /dev/null; then
        error "Cannot connect to database. Check DATABASE_URL environment variable."
    fi
    
    success "Database connection verified"
}

check_existing_data() {
    log "Checking for existing data..."
    
    local team_count=$(psql "$DATABASE_URL" -t -c "SELECT COUNT(*) FROM teams;" 2>/dev/null | tr -d ' ' || echo "0")
    local user_count=$(psql "$DATABASE_URL" -t -c "SELECT COUNT(*) FROM users;" 2>/dev/null | tr -d ' ' || echo "0")
    local doc_count=$(psql "$DATABASE_URL" -t -c "SELECT COUNT(*) FROM documents;" 2>/dev/null | tr -d ' ' || echo "0")
    
    if [[ "$team_count" -gt 0 || "$user_count" -gt 0 || "$doc_count" -gt 0 ]]; then
        warn "Existing data found: $team_count teams, $user_count users, $doc_count documents"
        
        if [[ "$FORCE" != true ]]; then
            echo ""
            echo "⚠️  WARNING: This will overwrite existing data!"
            echo "   Teams: $team_count"
            echo "   Users: $user_count" 
            echo "   Documents: $doc_count"
            echo ""
            read -p "Continue? (y/N): " -n 1 -r
            echo
            if [[ ! $REPLY =~ ^[Yy]$ ]]; then
                echo "Import cancelled."
                exit 0
            fi
        fi
    fi
}

backup_existing_data() {
    if [[ "$DRY_RUN" == true ]]; then
        log "[DRY RUN] Would backup existing database"
        return
    fi
    
    log "Creating backup of existing data..."
    
    local backup_file="/tmp/outline_backup_$(date +%Y%m%d_%H%M%S).sql"
    
    if pg_dump "$DATABASE_URL" > "$backup_file" 2>/dev/null; then
        success "Backup created: $backup_file"
    else
        warn "Failed to create backup, continuing anyway..."
    fi
}

import_database_sql() {
    local sql_file="$IMPORT_DIR/database.sql"
    
    if [[ ! -f "$sql_file" ]]; then
        log "No SQL file found, skipping SQL import"
        return
    fi
    
    if [[ "$DRY_RUN" == true ]]; then
        log "[DRY RUN] Would import database from: $sql_file"
        return
    fi
    
    log "Importing database from SQL: $sql_file"
    
    # Drop existing data if force is enabled
    if [[ "$FORCE" == true ]]; then
        log "Dropping existing tables..."
        psql "$DATABASE_URL" -c "DROP SCHEMA public CASCADE; CREATE SCHEMA public;" &>/dev/null || true
    fi
    
    if psql "$DATABASE_URL" < "$sql_file"; then
        success "Database imported from SQL"
    else
        error "Failed to import database from SQL"
    fi
}

import_database_json() {
    local json_file="$IMPORT_DIR/workspace.json"
    
    if [[ ! -f "$json_file" ]]; then
        log "No JSON file found, skipping JSON import"
        return
    fi
    
    if [[ "$DRY_RUN" == true ]]; then
        log "[DRY RUN] Would import database from: $json_file"
        return
    fi
    
    log "Importing database from JSON: $json_file"
    
    # Create a Node.js script to import data
    cat > "$IMPORT_DIR/import_script.js" << 'EOF'
const { Sequelize, DataTypes } = require('sequelize');
const fs = require('fs');

async function importData() {
    const sequelize = new Sequelize(process.env.DATABASE_URL, {
        dialect: 'postgres',
        logging: false
    });

    try {
        await sequelize.authenticate();
        console.log('Connected to database');

        const data = JSON.parse(fs.readFileSync('workspace.json', 'utf8'));
        console.log('Loaded export data from', data.exportedAt);

        // Import in dependency order
        const tables = [
            'teams', 'users', 'collections', 'groups', 'documents', 
            'attachments', 'shares', 'stars', 'pins', 'views', 
            'memberships', 'group_users'
        ];

        for (const table of tables) {
            if (data[table] && data[table].length > 0) {
                console.log(`Importing ${data[table].length} records to ${table}...`);
                
                // Clear existing data if any
                await sequelize.query(`TRUNCATE TABLE ${table} CASCADE`);
                
                // Insert new data
                for (const record of data[table]) {
                    const columns = Object.keys(record).join(', ');
                    const values = Object.values(record).map(v => 
                        v === null ? 'NULL' : `'${String(v).replace(/'/g, "''")}'`
                    ).join(', ');
                    
                    await sequelize.query(
                        `INSERT INTO ${table} (${columns}) VALUES (${values})`
                    );
                }
                
                console.log(`✓ Imported ${table}`);
            }
        }

        console.log('Import completed successfully');
        
    } catch (error) {
        console.error('Import failed:', error);
        process.exit(1);
    } finally {
        await sequelize.close();
    }
}

importData();
EOF

    cd "$IMPORT_DIR"
    if node import_script.js; then
        rm import_script.js
        success "Database imported from JSON"
    else
        error "JSON import script failed"
    fi
}

import_files() {
    if [[ "$SKIP_FILES" == true ]]; then
        log "Skipping file attachments import"
        return
    fi
    
    local files_dir="$IMPORT_DIR/files"
    
    if [[ ! -d "$files_dir" ]]; then
        log "No files directory found, skipping file import"
        return
    fi
    
    if [[ "$DRY_RUN" == true ]]; then
        log "[DRY RUN] Would import files from: $files_dir"
        return
    fi
    
    log "Importing file attachments from: $files_dir"
    
    # Default file storage location
    local storage_dir="/var/lib/outline/data"
    
    # Create storage directory if it doesn't exist
    mkdir -p "$storage_dir"
    
    if cp -r "$files_dir"/* "$storage_dir"/ 2>/dev/null; then
        # Fix permissions
        chown -R ubuntu:ubuntu "$storage_dir" 2>/dev/null || true
        chmod -R 755 "$storage_dir" 2>/dev/null || true
        
        local size=$(du -sh "$storage_dir" | cut -f1)
        success "Files imported ($size)"
    else
        warn "No files found to import or copy failed"
    fi
}

show_import_summary() {
    echo ""
    echo "📊 Import Summary"
    echo "=================="
    
    if [[ -f "$IMPORT_DIR/export_metadata.json" ]]; then
        echo "📅 Export Date: $(node -p "JSON.parse(require('fs').readFileSync('$IMPORT_DIR/export_metadata.json')).exportedAt" 2>/dev/null || echo 'Unknown')"
        echo "🏷️  Export Version: $(node -p "JSON.parse(require('fs').readFileSync('$IMPORT_DIR/export_metadata.json')).version" 2>/dev/null || echo 'Unknown')"
    fi
    
    if [[ "$SKIP_DATABASE" != true ]]; then
        local team_count=$(psql "$DATABASE_URL" -t -c "SELECT COUNT(*) FROM teams;" 2>/dev/null | tr -d ' ' || echo "0")
        local user_count=$(psql "$DATABASE_URL" -t -c "SELECT COUNT(*) FROM users;" 2>/dev/null | tr -d ' ' || echo "0")
        local doc_count=$(psql "$DATABASE_URL" -t -c "SELECT COUNT(*) FROM documents;" 2>/dev/null | tr -d ' ' || echo "0")
        local collection_count=$(psql "$DATABASE_URL" -t -c "SELECT COUNT(*) FROM collections;" 2>/dev/null | tr -d ' ' || echo "0")
        
        echo "👥 Teams: $team_count"
        echo "👤 Users: $user_count"
        echo "📚 Collections: $collection_count"
        echo "📄 Documents: $doc_count"
    fi
    
    if [[ "$SKIP_FILES" != true && -d "/var/lib/outline/data" ]]; then
        local file_size=$(du -sh "/var/lib/outline/data" 2>/dev/null | cut -f1 || echo "0B")
        echo "📎 Files: $file_size"
    fi
}

# Parse command line arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        -d|--dir)
            IMPORT_DIR="$2"
            shift 2
            ;;
        -f|--force)
            FORCE=true
            shift
            ;;
        --skip-files)
            SKIP_FILES=true
            shift
            ;;
        --skip-database)
            SKIP_DATABASE=true
            shift
            ;;
        --dry-run)
            DRY_RUN=true
            shift
            ;;
        -v|--verbose)
            VERBOSE=true
            shift
            ;;
        -h|--help)
            usage
            exit 0
            ;;
        *)
            error "Unknown option: $1"
            ;;
    esac
done

# Validate arguments
if [[ -z "$IMPORT_DIR" ]]; then
    error "Import directory is required. Use -d or --dir option."
fi

# Check for DATABASE_URL
if [[ -z "$DATABASE_URL" ]]; then
    error "DATABASE_URL environment variable is required."
fi

# Main execution
echo "📥 Starting Outline import..."
echo "📁 Import directory: $IMPORT_DIR"
echo "💪 Force mode: $FORCE"
echo "🏃 Dry run: $DRY_RUN"
echo ""

check_dependencies
validate_import_directory
check_database_connection

if [[ "$SKIP_DATABASE" != true ]]; then
    check_existing_data
    backup_existing_data
fi

# Import database
if [[ "$SKIP_DATABASE" != true ]]; then
    # Try SQL first, then JSON
    if [[ -f "$IMPORT_DIR/database.sql" ]]; then
        import_database_sql
    elif [[ -f "$IMPORT_DIR/workspace.json" ]]; then
        import_database_json
    else
        error "No database file found to import"
    fi
fi

# Import files
import_files

# Show summary
if [[ "$DRY_RUN" != true ]]; then
    show_import_summary
    echo ""
    success "Import completed successfully!"
    echo ""
    echo "🚀 You can now start Outline and access your imported data."
    echo "🔗 Don't forget to update any configuration if needed."
else
    echo ""
    success "Dry run completed - no changes made"
fi